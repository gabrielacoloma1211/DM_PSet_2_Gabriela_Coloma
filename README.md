# **Nombres:** Gabriela Coloma y Francisco Alarcon

# **Homework 4**

## **1. Foundational Research: Ecuadorian Data Sovereignty (LOPDP)**

Ecuador's Ley Orgánica de Protección de Datos Personales (LOPDP),
enforced starting in July 2023 , establishes comprehensive individual
rights over personal data. This module requires you to research the
specific mandates of this law as they relate to automated
decision-making and cross-border data management.

### **1.1. What are the three core principles (criterios mínimos) that govern all data processing activities under the LOPDP?** 

En el artículo 2 de la LOPDP no indican en qué casos la ley no será
aplicable y, en el punto f específicamente, nos dice lo siguiente: "En
cualquiera de estos casos deberá darse cumplimiento a los estándares
internacionales en la materia de derechos humanos y a los principios de
esta ley, y como mínimo a los criterios de legalidad, proporcionalidad y
necesidad" (Asamblea Nacional del Ecuador, 2021).

Siendo legalidad que se tiene que sostener en un sustento legal como
contrato, consentimiento, etc. La proporcionalidad significa que tiene
que ser proporcional lo que se busca encontrar como resultado con el
daño que le causa a la persona. Y por último la necesidad que nos indica
que básicamente si es que hay cualquier otra forma de probar o de lograr
el objetivo sin estos datos se deberá hacer y si no hay se debe usar
solo la información necesaria ni un poco más.

### **1.2. Locate the specific article (or section within an article) of the LOPDP that grants the data subject the right "to not be object of a decision based solely on automated valuations". Explain the details and protections it provides.**

Primero se presenta en el artículo 12, donde nos dicen que el dueño de
la información o el titular de esta información debe poder saber cómo,
por qué y todos los detalles de qué están haciendo con su información.
Aquí se menciona en el punto 14 que la persona tiene que poder saber de:
"La existencia y forma en que pueden hacerse efectivos sus derechos de
acceso, eliminación, rectificación y actualización, oposición,
anulación, limitación del tratamiento y a no ser objeto de una decisión
basada únicamente en valoraciones automatizadas" (Asamblea Nacional del
Ecuador, 2021).

También en el artículo 20 "Derecho a no ser objeto de una decisión
basada única o parcialmente en valoraciones automatizadas" (Asamblea
Nacional del Ecuador, 2021) nos indican que una persona tiene derecho a
no ser sometida a una decisión que fue hecha por procesos automáticos
como algoritmos, IA, profiling, scoring, etc. Estos pueden causar
efectos jurídicos o afectar a derechos que son fundamentales para los
seres humanos.

La persona se puede defender de esto de cinco maneras:

i. Se puede solicitar una explicación de por qué se tomó esa decisión,
la lógica detrás o los factores que influyeron dentro de esta decisión
al encargado del tratamiento de estos datos personales.

ii. Se pueden dar observaciones donde se puede corregir cosas que no
estén bien o agregar información de parte del individuo.

iii. Pedir que le indiquen cuáles son los criterios que usa el algoritmo
o el sistema, sobre cómo evalúa o qué valores usa y así.

iv. Se puede pedir saber cuáles son los datos que están utilizando al
usarlos en este proceso automatizado y también ver de dónde vino o de
dónde se sacó esta información, lo que puede ayudar a ver errores en el
proceso.

v. La persona puede "retar" la decisión, es decir, puede pedir que la
vuelvan a revisar con más control humano o que la corrijan o que la
reconsideren.

Por último, el derecho también se puede no aplicar cuando existe un
contrato que fuerce este procedimiento entre el individuo y la persona
que maneja los datos, como puede ser para la autorización de una tarjeta
de crédito. También cuando una ley o algo legal lo autorice, incluso el
consentimiento textual del individuo dueño de la información, como
escrito o verificable. O también cuando la decisión es trivial y no
afecta al individuo con su decisión. Y lo más importante es que dice que
no pueden hacer que renuncies a este derecho anticipadamente, como por
ejemplo si te hicieran firmar un contrato que dice que tú renuncias a
este derecho al entrar a la empresa o algo así (Asamblea Nacional del
Ecuador, 2021).

También el artículo 21 habla sobre este mismo derecho pero en niños y
niñas, que cumple con los mismos criterios que ya se explicaron, pero
que aparte dice que no se pueden tomar datos sensibles o delicados de
niños o niñas sin el consentimiento de los padres o tutores legales de
los mismos. A partir de sus 15 años ya podrían dar el consentimiento por
ellos mismos, pero igual verificable y explícito. También se pueden usar
si hay un interés público esencial, pero cumpliendo los derechos humanos
de los niños, los tres criterios mencionados en el punto anterior y que
incluya salvaguardas para que se protejan y se hagan cumplir tales
derechos ya mencionados (Asamblea Nacional del Ecuador, 2021).

### **1.3. In the context of an AI-driven system (e.g., hiring or loan approval), explain the operational impact of this right. How does this LOPDP provision compel a data controller to provide human intervention or oversight?** 

Primero que nada, como es un sistema de contratación o aprobación de
préstamos, ya que causarían efectos jurídicos, entra dentro del artículo
20 de la LOPDP (Asamblea Nacional del Ecuador, 2021). Después, como esta
nos indica, ninguna persona puede ser sometida a un proceso con una
decisión que sea hecha de forma automatizada. Eso quiere decir que, en
la empresa, el sistema de IA puede hacer recomendaciones o resúmenes,
pero no puede dar la decisión final sobre estos casos. Esto quiere decir
que en una parte del proceso de este sistema se tiene que tener
intervención humana significativa. O sea, para que sea significativa no
solo puede revisar y seguir, sino que tiene que participar del proceso.
Es decir, tiene que revisar el caso a conciencia y seguir con la
decisión de la IA o cambiarla si fuera el caso.

La persona que es encargada de este proceso tiene que estar lista para
las formas de defensa de los individuos. Por ejemplo, para cumplir con
la explicación motivada, que es el punto i de la pregunta anterior,
tiene que saber el porqué de la toma de esta decisión. También, para
cumplir con el punto ii y v, la persona encargada deberá poder alterar o
modificar esta decisión si se impugna, ya sea para cambiarla, analizarla
nuevamente, ingresar más información, etc.

Esto quiere decir que la ley obliga a la empresa, en realidad, a crear
un sistema que permita:

Explicar la toma de decisiones de la IA, como lo que mencionamos antes,
factores a considerar, qué datos se tomaron, de dónde se tomaron los
datos, cuáles son los criterios que usa este programa, cómo se puede
apoyar el individuo para que se hagan cumplir estas especificaciones.
Esto quiere decir que debe ser un programa transparente, que lleve
trazabilidad y documentación de lo que va realizando.

Esto nos lleva al siguiente punto, que es otorgar al cliente una forma
de presentar todos los tipos de defensa contra el saltarse este derecho.
Como puede ser tener apertura para presentar observaciones, corregir
información, aumentar datos. Esto implica tener contacto directo entre
la empresa y el individuo de una forma accesible claramente, y también
un proceso interno de la empresa para manejar esta revisión que sea
conformado por humanos que estén capacitados y listos para poder tratar
con este tipo de casos como el de la reevaluación 100% humana.

.

### **1.4. Under what conditions is the international transfer of personal data restricted by the LOPDP?** 

Para responder a esta pregunta nos vamos a basar en el capítulo 9 que se
llama "Transferencia o comunicación internacional de datos personales"
(Asamblea Nacional del Ecuador, 2021).

En términos generales, la transferencia de datos es permitida según el
artículo 56 a países, personas o entidades que tengan los mismos niveles
de protección que los nuestros, es decir, los del Ecuador. Para esto
entra la Autoridad de Protección de Datos Personales, que será la que
nos indica si se cumple o no con los niveles de seguridad (Asamblea
Nacional del Ecuador, 2021).

La transferencia de datos es restringida cuando el país no cumple con
las medidas de seguridad requeridas por la Autoridad de Protección de
Datos Personales, sin embargo, se puede realizar si es que, como dice el
artículo 57, el encargado del tratamiento de estos datos puede cumplir
con asegurar los mismos niveles de protección que el Ecuador, asegurar
que haya cómo reclamar o tomar acciones judiciales y, si es que es el
caso, dar una reparación integral (Asamblea Nacional del Ecuador, 2021).

O sea, no está permitida la transferencia de datos cuando no se cumplen
con las mismas normas o estándares del Ecuador, no se prometen garantías
adecuadas y no se tiene autorización de la Autoridad de Protección de
Datos Personales. También se deberá cumplir con el artículo 21 que vimos
antes sobre menores de edad y su debido consentimiento explícito, y que
cumpla el artículo 12 que dice "El titular de datos personales tiene
derecho a ser informado conforme los principios de lealtad y
transparencia"(Asamblea Nacional del Ecuador, 2021) sobre para qué es la
información, cómo la obtuvieron, por cuánto tiempo, etc.

### **1.5. Explain the role of the Data Protection Authority (DPA) regarding international data transfers. How does this requirement create operational friction or regulatory compliance barriers for a multinational AI company that typically relies on centralized cloud infrastructure outside of Ecuador?** 

Primero que nada, en el artículo 4 nos indican que la Autoridad de
Protección de Datos Personales (APD) es "Autoridad pública independiente
encargada de supervisar la aplicación de la presente ley, reglamento y
resoluciones que ella dicte, con el fin de proteger los derechos y
libertades fundamentales de las personas naturales, en cuanto al
tratamiento de sus datos personales"(Asamblea Nacional del Ecuador,
2021).

Ahora, igual basándonos en el capítulo IX de la LOPDP (Asamblea Nacional
del Ecuador, 2021), podemos ver en lo dicho anteriormente que, según el
artículo 56, primero nos indican que la transferencia sí se puede hacer
a países que cumplen con ciertas especificaciones que siguen con las
medidas de protección de datos del Ecuador. La APD es la encargada de
declarar cuál es el nivel adecuado que deben tener estos países. En el
artículo nos dice que la APD deberá emitir una resolución motivada que
sea la que indica que cumplen con los requisitos y leyes que rigen en el
Ecuador para que este país externo sea considerado para el intercambio
de datos personales. También en el artículo 61 nos dicen que este
permiso para tales países puede ser revocado por la APD, ya que la misma
realizará estudios constantes para ver la realidad del país, que es
cambiante, para ver si sigue cumpliendo con los estándares.

Este mismo artículo nos dice que la APD también "podrá implementar
métodos de control ex post que serán definidos en el Reglamento a la
Ley" (Asamblea Nacional del Ecuador, 2021). Esto quiere decir que puede
tratar y ver lo que está pasando con una transferencia de datos que ya
fue realizada y puede mantener comunicación con el otro país para poder
eliminar cualquier riesgo de la violación de la protección de datos.

Ahora, en el artículo 57 nos dicen que, si el país no fue considerado
por la APD para el intercambio de datos, como se menciona en el literal
anterior, igual puede haber intercambio de datos siempre y cuando se
cumpla con unas condiciones y también que debe haber un instrumento
jurídico vinculante en el que se deberán tomar en cuenta
especificaciones directas de la APD (Asamblea Nacional del Ecuador,
2021).

En el artículo 58 nos hablan de las normas corporativas, que básicamente
es cuando en una empresa hay transferencia de datos internacional.
Entonces se tiene que poder crear un propio set de reglas para esta
transferencia, pero estas reglas deben ser aprobadas y tienen que seguir
el formato que dice la APD, que nos indica los estándares y las reglas
del procedimiento a cumplir para que sea válido y que cumpla con lo que
nos dice la LOPDP, como también ser notificado en caso de cualquier
cambio o alteración (Asamblea Nacional del Ecuador, 2021).

En el artículo 59, de igual forma, nos dicen que, si es que no entra en
ninguno de los casos anteriores, entonces la transferencia solo se podrá
hacer con una autorización directa de la APD. Esto quiere decir que la
APD deberá revisar el caso específico y otorgar o no un permiso para
esta transferencia (Asamblea Nacional del Ecuador, 2021).

El problema para las empresas multinacionales de IA es que normalmente,
como nos indica el enunciado, los datos se envían automáticamente a
servidores internacionales. Pero por la regulación ecuatoriana esta
transferencia requiere de aprobación de la APD para poder hacerlo, lo
que hace que se generen demoras, ya que se debe validar en qué caso cae
de los anteriores mencionados o bajo qué circunstancia se pueden
transferir. Esto nos lleva al siguiente punto, que es la parte legal, ya
que para estas aprobaciones se deberán obtener permisos legales de la
APD y la empresa deberá también modificar ciertos contratos o
especificaciones para cumplir con el estándar. Esto, aparte de ser
costoso, ya que se necesitarían abogados y comunicación, se puede
demorar y demorar todo el proceso de la empresa. También puede llevar a
que la empresa tenga que cambiar arquitectura para que haya cumplimiento
de normas, como de no ser automático si fuera el caso. Significaría que
la empresa tiene que contar con la opción de poder tener controles por
parte de la APD, lo que requiere mantener información sobre la
utilización de los datos que usa la empresa de IA, lo que igualmente
puede ser costoso y requerir de sistemas de monitoreo continuo. También
se tiene una cierta incertidumbre porque la APD siempre puede retirar
los permisos a países, entonces generaría problemas para la empresa ya
que podría hasta tener que detener operaciones para solucionar los
permisos.

## **2. Corporate Policy Scrutiny: The Data Repurposing Conflict**

Major generative AI companies frequently face scrutiny for collecting
and repurposing user data (such as chat inputs) for model training, a
practice that directly challenges the principles of purpose limitation
and explicit consent in global privacy laws.

### **2.1. Select two major generative AI providers (e.g., OpenAI, Meta, or Anthropic). Briefly summarize how each company differentiates the data usage practices between their Enterprise/API Services (for paying business clients) and their Consumer/Chatbot Services (for free public users) regarding model training.** 

Primero, las dos IAs utilizadas para este ejercicio son OpenAI y
Anthropic. La información sale de las páginas oficiales de ambas
empresas.

Ahora, OpenAI para su Enterprise/API Services en realidad no usa los
datos que el usuario le da para entrenar modelos de forma automática. La
página dice que sí hay forma de hacerlo si decides hacerlo
explícitamente en la configuración, pero que en términos generales no se
usa tu información recolectada, ya sean datos, prompts, outputs, para
entrenar al modelo ni para mejorarlo. Nos habla hasta de encriptación
usando AES-256 y TLS 1.2 en tránsito para la seguridad de la información
prestada (OpenAI, 2025a).

En la versión gratuita o Consumer/Chatbot, por otro lado, nos indica la
página web que en realidad sí usa los datos para entrenar al modelo.
Como dice en la página "we may use Content you provide us to improve our
Services, for example to train the models that power ChatGPT", o sea que
sí usa la información que le mandamos como archivos e inputs y outputs
para entrenamiento de modelos. Dice también claramente que puedes
seleccionar no dar esta información y "opt-out" de dar la información
para este entrenamiento (OpenAI, 2025b).

Ahora, hablando de Anthropic, primero de la Enterprise/API, que en este
caso sería Claude for Work y Anthropic API, nos dice su página web que
los datos que se ingresan y el output que da no se usan por defecto para
poder entrenar sus modelos. Como en todos los casos, igual dice que si
es que se busca o si se reportan explícitamente errores o cualquier tipo
de recomendación, entonces sí se puede usar esa información para
entrenar los modelos. Igual, si es que se elige dar la información
voluntariamente, igual se pueden dar los datos (Anthropic, 2025b).

En el caso de Anthropic Consumer/Chatbot, entonces estaríamos hablando
de Claude free, pro y max. Entonces el caso especifica que se usarán los
chats y las sesiones de código si es que eliges entregar los datos de
estos chats, también si es que está alguna señal de tipo de peligro en
el chat o si se decide entrar en uno de sus programas que tienen para
que la gente les ayude a entrenar modelos. También especifica que en
ninguno de los casos se utilizan los datos para entrenamiento del modelo
si es que se utiliza en modo incógnito (Anthropic, 2025a).

### **2.2. Analyze the public-facing policy for the consumer chat version of one of your selected companies. Identify the type of user input data (e.g., chat content, account info, technical data) that may be used for model training.** 

La política de privacidad de OpenAI para servicios consumer identifica
cuatro categorías de datos potencialmente utilizables para mejorar
modelos (OpenAI Privacy Policy, 2024):

#### **Contenido del usuario** 

Esta categoría abarca todos los prompts y mensajes escritos en ChatGPT,
las respuestas generadas por el modelo, archivos subidos (PDFs,
imágenes, documentos de audio, código fuente), y crucialmente, el
feedback explícito. OpenAI especifica: \"If you choose to provide
feedback, the entire conversation associated with that feedback may be
used to train our models\" (OpenAI Help Center, 2025), lo cual significa
que incluso usuarios que desactivaron el entrenamiento pueden
inadvertidamente contribuir datos si utilizan los botones de valoración.

#### **Información de cuenta**

Incluye dirección de email, credenciales de cuenta, número de teléfono,
información de pago para suscriptores Plus, y fecha de creación de
cuenta. Aunque OpenAI indica que toma medidas para reducir información
personal identificable antes del entrenamiento, la recopilación inicial
es exhaustiva.

#### **Datos técnicos y metadatos**

OpenAI recolecta dirección IP, tipo y versión de navegador, sistema
operativo, identificadores de dispositivo, geolocalización aproximada,
cookies, timestamps de interacción, y patrones de uso como duración de
sesión y frecuencia. Estos metadatos, aunque aparentemente inocuos,
pueden permitir re-identificación cuando se combinan con otros datasets.

#### **Información de comunicaciones**

Comprende el contenido de correos electrónicos enviados a OpenAI y
comentarios en redes sociales oficiales. Esta categoría extiende la
superficie de recopilación más allá de la interfaz principal del
chatbot.

OpenAI aclara que \"we retain certain data from your interactions with
us, but we take steps to reduce the amount of personal information in
our training datasets before they are used to improve and train our
models\" (OpenAI Help Center, 2025), aunque no especifica qué técnicas
de anonimización emplea ni su efectividad demostrada.

### **2.3. Describe the practical process a user must follow to opt out of having their data used for training (e.g., submission of a form, navigation of settings, or use of a specific toggle).** 

OpenAI ofrece tres métodos para que usuarios eviten que sus datos sean
utilizados para entrenamiento:

#### **Método 1: Toggle en configuraciones** 

El proceso más directo requiere:

1.  Iniciar sesión en ChatGPT

2.  Hacer clic en el ícono de perfil (esquina superior derecha)

3.  Seleccionar \"Settings\" (Configuraciones)

4.  Navegar a \"Data Controls\" (Controles de datos)

5.  Desactivar el toggle \"Improve the model for everyone\"

OpenAI confirma: \"Once you opt out, new conversations will not be used
to train our models\" (OpenAI Help Center, 2025). Notablemente, el
historial de conversaciones se mantiene visible incluso con esta opción
desactivada.

#### **Método 2: Privacy Portal**

Alternativamente, usuarios pueden acceder a privacy.openai.com, iniciar
sesión, seleccionar \"Make a Privacy Request\", y elegir la opción \"Do
not train on my content\". Este método genera confirmación por email
indicando: \"We will no longer use your content to train our models. As
a reminder, this request is forward looking and does not apply to
content that had previously been disassociated from your account\"
(OpenAI, 2025b).

#### **Método 3: Chats temporales**

Para conversaciones específicas que no deben ser utilizadas, existe el
modo Temporary Chat accesible desde un ícono en la esquina superior
derecha de la pantalla de chat. Estos chats no aparecen en historial, no
crean memorias, y nunca se utilizan para entrenamiento según la
documentación oficial.

#### **Limitaciones críticas del opt-out**

El opt-out presenta tres restricciones significativas:

1.  No retroactividad: \"The opt-out isn\'t retroactive. Any data
    you\'ve already contributed in the past may still reside in
    training datasets\" (OpenAI Help Center, 2025).

2.  Excepción de feedback: Incluso con opt-out activo, \"If you choose
    to provide feedback, the entire conversation associated with that
    feedback may be used to train our models\" (OpenAI Help Center,
    2025).

3.  Retención de seguridad: \"When chat history is disabled, we will
    retain new conversations for 30 days and review them only when
    needed to monitor for abuse\" (OpenAI, 2023).

### **2.4. Critically evaluate: How does this opt-out mechanism conflict with the LOPDP's mandate for prior, informed, and explicit consent for data processing?** 

La LOPDP ecuatoriana, vigente desde julio de 2023, establece en su
artículo 7 que el tratamiento de datos personales requiere el
consentimiento previo, expreso, informado e inequívoco del titular
(Asamblea Nacional del Ecuador, 2021). Este requisito genera un
conflicto estructural con los mecanismos de opt-out predominantes en la
industria de IA generativa.

#### **El problema fundamental: inversión de la carga del consentimiento**

Los modelos de opt-out operan bajo una presunción de consentimiento
implícito: el usuario acepta el tratamiento hasta que activamente lo
rechace. La LOPDP, siguiendo el modelo europeo del GDPR, exige lo
contrario: el tratamiento solo es lícito cuando existe manifestación
previa y afirmativa de voluntad. La LOPDP establece en su artículo 7 que
el tratamiento requiere el consentimiento del titular, el cual debe
cumplir requisitos específicos definidos en el artículo 8: debe ser
libre (exento de vicios), específico (determinación concreta de medios y
fines), informado (cumpliendo el principio de transparencia) e
inequívoco (sin dudas sobre el alcance de la autorización).

En el caso de OpenAI, el entrenamiento está habilitado por defecto para
usuarios gratuitos y Plus. Esto significa que un usuario ecuatoriano que
crea una cuenta y comienza a usar ChatGPT inmediatamente contribuye
datos para entrenamiento, sin haber otorgado consentimiento explícito
para este propósito específico. La aceptación de términos de servicio
generales no satisface el requisito de especificidad que demanda la
LOPDP.

#### **Violación del principio de información previa**

El artículo 12 de la LOPDP establece el derecho a la información,
requiriendo que el titular sea informado sobre los fines del
tratamiento, la base legal, tipos de tratamiento, tiempo de
conservación, y otros aspectos relevantes conforme los principios de
lealtad y transparencia. Sin embargo, las políticas de OpenAI y
Anthropic presentan información sobre uso de datos para entrenamiento en
documentos extensos de términos de servicio que la mayoría de usuarios
no lee. El descubrimiento de que LinkedIn activó silenciosamente el uso
de datos para IA en agosto de 2024, con usuarios descubriéndose semanas
después a través de medios de comunicación (De La Torre v. LinkedIn
Corp., 2025), ilustra cómo la industria frecuentemente prioriza la
adopción sobre la transparencia.

#### **Tensión con el derecho de oposición efectivo**

El artículo 16 de la LOPDP reconoce el derecho de oposición al
tratamiento de datos. Sin embargo, ambas compañías reconocen que el
opt-out no es retroactivo: los datos ya incorporados en modelos
entrenados no pueden removerse. Anthropic lo expresa directamente:
*\"*Your data will still be included in model training that has already
started and in models that have already been trained*\"* (Anthropic,
2025a). Esta irreversibilidad contradice el principio de que el
consentimiento debe poder retirarse *\"*en cualquier momento*\"* con
efectos plenos, como estipula el artículo 8 de la LOPDP.

#### **Implicaciones para usuarios ecuatorianos**

Un ciudadano ecuatoriano que utilice ChatGPT se encuentra ante una
situación jurídicamente problemática: sus datos pueden estar siendo
procesados para entrenamiento de modelos sin consentimiento válido bajo
legislación ecuatoriana, y su posterior opt-out no remedia completamente
la violación inicial. La LOPDP faculta a la Autoridad de Protección de
Datos Personales para imponer sanciones de hasta 1% de la facturación
global de empresas infractoras, aunque la aplicación extraterritorial
presenta desafíos prácticos significativos.

### **2.5. Legal Risk Scenarios: Research a recent legal challenge or public controversy where an AI company (e.g., Meta, LinkedIn, or Amazon) was accused of using previously collected user-generated content or biometric data for a new, secondary AI training purpose without proper consent. Briefly summarize the nature of the alleged violation (e.g., biometric privacy, repurposing of communication data).** 

#### **Naturaleza de la controversia**

En mayo de 2025, la organización europea de derechos digitales NOYB
envió una carta de cese y desistimiento a Meta Platforms Ireland
Limited, amenazando con iniciar una demanda colectiva europea por el uso
de datos personales de usuarios de Facebook e Instagram para entrenar
modelos de IA, incluyendo Llama (NOYB, 2025).

La denuncia documenta que Meta planificaba, desde el 27 de mayo de 2025,
utilizar publicaciones públicas de adultos en la Unión Europea
---incluyendo contenido que se remonta a 2007, imágenes privadas, y
datos de tracking--- para entrenar sus modelos de IA generativa. La base
legal invocada por Meta fue el \"interés legítimo\", evadiendo el
requisito de consentimiento explícito opt-in que exige el GDPR para este
tipo de procesamiento de alto riesgo.

#### **Naturaleza específica de la violación alegada**

La violación principal constituye una reutilización de datos para
propósito secundario no consentido. Los usuarios de Facebook e Instagram
proporcionaron originalmente sus datos para interacción social, no para
entrenamiento de sistemas de IA. El Reglamento General de Protección de
Datos (GDPR) establece en su artículo 6(4) que cuando el tratamiento
tiene un propósito diferente al original, requiere nuevo consentimiento
específico.

Adicionalmente, NOYB argumenta que una vez los datos se incorporan en
modelos de código abierto como Llama, se vuelve técnicamente imposible
cumplir con derechos fundamentales del GDPR como el derecho al olvido
(artículo 17) y el derecho de rectificación (artículo 16). Como expresó
Max Schrems, fundador de NOYB: *\"Once personal data has become part of
an AI model, there is no way to remove it\"* (NOYB, 2025).

#### **Estado actual y potencial impacto**

En junio de 2024, la Comisión de Protección de Datos de Irlanda (DPC)
había solicitado una suspensión temporal del entrenamiento de IA de
Meta. Sin embargo, en mayo de 2025, la misma DPC autorizó a Meta a
reiniciar el entrenamiento con ciertas condiciones, decisión que NOYB
calificó como *\"half-hearted\"* (NOYB, 2025). La demanda colectiva
potencial podría involucrar hasta 400 millones de usuarios europeos con
daños estimados de hasta €200 mil millones.

Este caso es particularmente relevante para el análisis bajo LOPDP
porque ilustra cómo datos recopilados para un propósito específico
pueden ser reutilizados años después para entrenar IA sin consentimiento
renovado ---una práctica que violaría directamente el principio de
finalidad establecido en el artículo 10 de la ley ecuatoriana.

## **3. Technical Risk Assessment and Mitigation**

AI systems, especially those processing sensitive data, require robust
security and governance to prevent privacy harms and data access risks.
This module focuses on threat modeling and risk management.

### **3.1. Choose one high-risk application and detail a specific scenario where it could lead to severe harm or unauthorized data access**

#### **Escenario específico de daño severo**

Consideremos la implementación de un chatbot de IA para triaje médico
inicial en un sistema de salud pública latinoamericano. El sistema
recibe síntomas de pacientes y sugiere prioridades de atención o
posibles diagnósticos preliminares.

**Escenario de daño**: Un paciente de 45 años reporta al chatbot dolor
torácico leve, fatiga y malestar general. El sistema, entrenado
predominantemente con datos de poblaciones caucásicas, interpreta estos
síntomas como ansiedad o condición musculoesquelética menor, asignando
baja prioridad. En realidad, el paciente presenta síntomas atípicos de
infarto agudo de miocardio ---presentación más común en poblaciones
latinas y en mujeres que el dolor torácico clásico. El retraso en
atención resulta en daño cardíaco permanente o muerte.

Este escenario ilustra múltiples vectores de riesgo:

i.  **Sesgo en datos de entrenamiento**: La investigación documenta que
    algoritmos médicos de IA frecuentemente subrrepresentan
    poblaciones no-caucásicas, resultando en menor precisión
    diagnóstica para estos grupos (Obermeyer et al., 2019).

ii. **Automatización de decisiones de alto impacto**: El artículo 20 de
    la LOPDP establece el derecho a no ser objeto de una decisión
    basada única o parcialmente en valoraciones automatizadas,
    incluida la elaboración de perfiles, que produzcan efectos
    jurídicos o que atenten contra los derechos y libertades
    fundamentales del titular.

iii. **Acceso no autorizado a datos sensibles**: El artículo 25 de la
     LOPDP establece las categorías especiales de datos personales,
     incluyendo entre ellos los datos sensibles y los datos de salud.
     El artículo 26 regula el tratamiento de datos sensibles,
     prohibiéndolo salvo en circunstancias específicas como el
     consentimiento explícito del titular o necesidad para proteger
     intereses vitales. Un chatbot médico con memorización de modelo
     (ver sección 3c) podría inadvertidamente revelar información de
     salud de otros pacientes.

### **3.2. Assume we deploy a new AI-driven predictive policing system deployed in a major Latin American city. Describe how historical bias present in the training data (e.g., police reporting practices) could cause the system to disproportionately target and surveil marginalized communities , resulting in discriminatory legal effects (a violation of LOPDP's principle of proportionality and the right to object to automated decisions ).** 

#### **Escenario: Sistema de vigilancia predictiva en ciudad latinoamericana**

Supongamos el despliegue de un sistema de policiamiento predictivo
similar a PredPol o Palantir en una ciudad importante de América Latina.
El sistema utiliza datos históricos de arrestos, denuncias, y reportes
policiales para generar \"zonas calientes\" (*hotspots*) donde se
predice mayor probabilidad de crímenes, y/o \"puntajes de riesgo\" para
individuos específicos.

#### **Mecanismo de perpetuación del sesgo histórico**

Los datos de entrenamiento reflejan décadas de prácticas policiales
discriminatorias documentadas en la región. En Brasil, por ejemplo, el
76.6% de víctimas de homicidio son afrobrasileños (Amnesty
International, 2025).La ONG Temblores documentó 4,687 casos de actos
violentos por parte de las fuerzas públicas entre el 28 de abril y el 26
de junio de 2021 durante las protestas en Colombia, incluyendo 44
homicidios, 1,468 víctimas de violencia física policial, y 1,832
detenciones arbitrarias (Temblores, 2021).

El ciclo de retroalimentación (*feedback loop*) opera así:

1.  **Datos históricos sesgados**: Barrios marginalizados muestran
    mayores tasas de arresto no porque cometan más crímenes, sino
    porque históricamente reciben mayor vigilancia policial.

2.  **Predicción algorítmica**: El sistema identifica estos mismos
    barrios como \"alto riesgo\" basándose en datos históricos de
    arrestos.

3.  **Incremento de patrullaje**: Se despliegan más oficiales a estas
    zonas, resultando en más arrestos (incluso por infracciones
    menores que pasarían desapercibidas en otras zonas).

4.  **Refuerzo del sesgo**: Los nuevos arrestos \"confirman\" las
    predicciones del algoritmo, creando datos que perpetúan el ciclo.

#### **Evidencia empírica de discriminación algorítmica**

El estudio de ProPublica sobre COMPAS (2016), el sistema de evaluación
de riesgo de reincidencia más utilizado en Estados Unidos, encontró que
personas negras incorrectamente clasificadas como \"alto riesgo\"
representaban el 45% de casos, versus solo 23% para personas blancas que
fue casi el doble de falsos positivos (Angwin et al., 2016). El estudio
de Lum e Isaac (2016) sobre PredPol en Oakland demostró que barrios
negros serían vigilados al doble de tasa que barrios blancos para
delitos de drogas, pese a que encuestas de salud indican tasas de
consumo similares entre grupos raciales.

#### **Casos en América Latina**

En Brasil, la implementación de reconocimiento facial en Salvador de
Bahía ha resultado en múltiples detenciones de inocentes, incluyendo un
guardia de seguridad detenido por \"95% de similitud\" con un criminal
buscado pese a nunca haber sido procesado (R3D, 2023). La Operación
Contención de noviembre de 2025 en favelas de Río de Janeiro, que
utilizó drones con reconocimiento facial, resultó en 121 muertes, la
operación más letal en la historia del estado, con víctimas
mayoritariamente negras y pobres (Amnesty International, 2025).

En Colombia, documentos filtrados del Comando General de Fuerzas
Militares revelan interés en identificar \"amenazas\" en redes sociales,
mientras la Fundación Karisma ha denunciado uso de IA para vigilar
manifestantes durante las protestas de 2021, sin marcos regulatorios
claros ni protocolos de derechos humanos (Fundación Karisma, 2023).

#### **Violaciones específicas a la LOPDP**

Este escenario viola múltiples disposiciones de la LOPDP:

a.  **Principio de proporcionalidad** (artículo 10, literal f): El
    tratamiento masivo de datos para vigilancia no es proporcional al
    fin perseguido cuando resulta en discriminación sistemática.

b.  **Derecho a no ser objeto de decisiones automatizadas** (artículo
    20): Las \"listas de vigilancia\" y \"puntajes de riesgo\"
    generados algorítmicamente producen efectos jurídicos
    significativos (mayor escrutinio policial, detenciones) basados
    únicamente en valoraciones automatizadas.

c.  **Prohibición de discriminación:** El uso de variables proxy como
    ubicación geográfica, nivel educativo, o historial de detenciones
    que correlacionan con raza/etnia constituye discriminación
    indirecta, violando el principio de igualdad establecido en la
    Constitución del Ecuador (Art. 11) y los principios de la LOPDP
    que exigen tratamiento de datos con estricto apego a derechos
    fundamentales

### **3.3. Explain the security threat known as \"Model Memorization\" or \"Data Leakage\" in generative AI. If an AI chatbot (used internally by a hospital) accidentally memorizes and reveals unmasked patient health information (PHI) shared during a training prompt, what specific privacy right under the LOPDP would be violated?** 

#### **Definición técnica del fenómeno**

La memorización en modelos de lenguaje es el fenómeno por el cual un LLM
almacena y puede reproducir fragmentos específicos de sus datos de
entrenamiento, en lugar de simplemente aprender patrones lingüísticos
generales (Carlini et al., 2021). Investigadores de Carnegie Mellon
University definen operacionalmente que *\"una frase presente en los
datos de entrenamiento está memorizada si podemos hacer que el modelo
reproduzca esa frase usando un prompt (mucho) más corto que la frase
misma\"* (CMU ML Blog, 2024).

#### **Mecanismo de ocurrencia**

La memorización ocurre debido a varios factores técnicos:

1.  **Duplicación de datos**: Cuando texto aparece múltiples veces en el
    dataset de entrenamiento, aumenta la probabilidad de memorización.
    Google DeepMind encontró que un texto específico aparecía más de
    40,000 veces en el dataset C4 de Common Crawl (Carlini et al.,
    2022).

2.  **Tamaño del modelo**: Investigaciones demuestran que modelos más
    grandes memorizan más datos que modelos más pequeños, incluso con
    la misma cantidad de datos de entrenamiento (Carlini et al.,
    2021).

3.  **Arquitectura**: Los módulos de atención en los bloques más
    profundos del transformer son principalmente responsables de la
    memorización, mientras que bloques anteriores contribuyen más a la
    generalización (Menta et al., 2025).

#### **Caso demostrado: \"Divergence Attack\" contra ChatGPT**

En 2023, investigadores de Google DeepMind demostraron el \"divergence
attack\": al pedir a ChatGPT que repitiera una palabra indefinidamente,
el modelo eventualmente \"diverge\" de su comportamiento normal y
comienza a emitir texto memorizado de entrenamiento. Este ataque extrajo
gigabytes de datos, con aproximadamente 5% del output siendo copia
exacta de 50+ tokens consecutivos, incluyendo direcciones de email,
números de teléfono, fragmentos de código de hasta 1,450 líneas, y
contenido NSFW (Nasr et al., 2023).

#### **Escenario hospitalario y PHI**

Si un hospital implementa un chatbot de IA interno para consultas
médicas, y durante su configuración o fine-tuning se expone a
información de salud protegida (PHI) de pacientes sin anonimización
adecuada, existe riesgo demostrado de que el modelo memorice y
posteriormente revele esta información.

Un caso documentado en Ontario, Canadá (2024) ilustra el riesgo: un bot
de Otter.ai que un médico había instalado en su dispositivo personal se
unió automáticamente a una reunión virtual de hepatología, grabó la
discusión sobre 7 pacientes, y envió resúmenes incluyendo nombres,
diagnósticos, y planes de tratamiento a 65 usuarios ---incluyendo 12 que
ya no trabajaban en el hospital (Canadian Healthcare Technology, 2024).

#### **Derecho específico violado bajo LOPDP**

La revelación inadvertida de PHI memorizada por un chatbot hospitalario
violaría el principio de confidencialidad establecido en el artículo 10,
literal g) de la LOPDP, que establece que \"el tratamiento de datos
personales debe concebirse sobre la base del debido sigilo y secreto, es
decir, no debe tratarse o comunicarse para un fin distinto para el cual
fueron recogidos\" salvo causales legales.

Adicionalmente, se violarían:

-   **Artículo 24 - Datos sensibles**: Los datos de salud requieren
    protecciones especiales y solo pueden tratarse bajo circunstancias
    específicas listadas en la ley. La memorización y posterior
    revelación no autorizada no encaja en ninguna excepción.

-   **Artículo 37 - Seguridad de datos personales**:El responsable o
    encargado del tratamiento debe \"implementar un proceso de
    verificación, evaluación y valoración continua y permanente de la
    eficiencia, eficacia y efectividad de las medidas de carácter
    técnico, organizativo y de cualquier otra índole, implementadas
    con el objeto de garantizar y mejorar la seguridad del tratamiento
    de datos personales\". Un modelo que puede revelar PHI demuestra
    falta de medidas de seguridad adecuadas.

-   **Artículo 43 - Notificación de vulneración de seguridad**: \"El
    responsable del tratamiento deberá notificar la vulneración de la
    seguridad de datos personales a la Autoridad de Protección de
    Datos Personales y la Agencia de Regulación y Control de las
    Telecomunicaciones, tan pronto sea posible, y a más tardar en el
    término de cinco (5) días después de que haya tenido constancia de
    ella\". Adicionalmente, el artículo 46 requiere notificación al
    titular \"sin dilación\" cuando la vulneración conlleve riesgo a
    sus derechos fundamentales.

La investigación técnica confirma que incluso con técnicas de
anonimización, algoritmos pueden re-identificar el 85.1% de adultos en
estudios de cohorte (Sweeney et al., 2018), lo que significa que la
simple eliminación de identificadores directos no elimina el riesgo de
memorización de información re-identificable.

## Referencias

- Amnesty International. (2025a). *Automated racism: How police data and algorithms code discrimination into policing*. Amnesty International UK.  
  https://www.amnesty.org.uk/files/2025-02/Automated%20Racism%20Report%20-%20Amnesty%20International%20UK%20-%202025.pdf

- Amnesty International. (2025b, noviembre). *Brazil: Rio de Janeiro police killed 121 people in horrific two-day operation*.

- Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016, mayo 23). *Machine bias: There's software used across the country to predict future criminals. And it's biased against blacks*. ProPublica.  
  https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

- Anthropic. (2025a, agosto 28). *Updates to consumer terms and privacy policy*. Anthropic News.  
  https://www.anthropic.com/news/updates-to-our-consumer-terms

- Anthropic. (2025b). *Is my data used for model training?*  
  https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training

- Anthropic. (2025c). *Is my data used for model training?*  
  https://privacy.claude.com/en/articles/7996868-is-my-data-used-for-model-training

- Asamblea Nacional del Ecuador. (2021). *Ley Orgánica de Protección de Datos Personales*. Quinto Suplemento del Registro Oficial No. 459, 26 de mayo de 2021.

- Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., & Raffel, C. (2021). *Extracting training data from large language models*. Proceedings of the 30th USENIX Security Symposium, 2633–2650.

- *De La Torre v. LinkedIn Corp.*, No. 5:25-cv-00709 (N.D. Cal. Jan. 21, 2025).

- Federal Trade Commission. (2023, mayo). *FTC says Ring employees illegally surveilled customers, failed to stop hackers from taking control of users' cameras*.  
  https://www.ftc.gov/news-events/news/press-releases/2023/05/ftc-says-ring-employees-illegally-surveilled-customers-failed-stop-hackers-taking-control-users

- Garante per la Protezione dei Dati Personali. (2024, diciembre 20). *OpenAI: Italian SA fines company €15 million*.  
  https://www.garanteprivacy.it/

- Hartmann, V., Suri, A., Bindschaedler, V., Evans, D., Tople, S., & West, R. (2023). *SoK: Memorization in general-purpose large language models*. arXiv preprint arXiv:2310.18362.

- Temblores ONG. (2021). *Protests and Human Rights Violations in Colombia: Second Alert*.  
  https://www.wola.org/2021/07/protests-and-human-rights-violations-in-colombia-second-alert/

- Lum, K., & Isaac, W. (2016). *To predict and serve?* Significance, 13(5), 14–19.

- Menta, T. R., et al. (2025). *Analyzing memorization in large language models through the lens of model attribution*. arXiv preprint arXiv:2501.05078.

- Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E., Tramèr, F., & Lee, K. (2023). *Scalable extraction of training data from (production) language models*. arXiv preprint arXiv:2311.17035.

- NOYB. (2025, mayo 14). *NOYB sends Meta “cease and desist” letter over AI training. European class action as potential next step*.  
  https://noyb.eu/en/noyb-sends-meta-cease-and-desist-letter-over-ai-training-european-class-action-potential-next-step

- Obermeyer, Z., Powers, B., Vogeli, C., & Mullainathan, S. (2019). *Dissecting racial bias in an algorithm used to manage the health of populations*. Science, 366(6464), 447–453.

- OpenAI. (2025a). *Business data privacy, security, and compliance*.  
  https://openai.com/business-data/

- OpenAI. (2025b). *Enterprise privacy*.  
  https://openai.com/enterprise-privacy/

- OpenAI. (2025c). *Privacy policy*.  
  https://openai.com/policies/privacy-policy/

- OpenAI Help Center. (2025). *How your data is used to improve model performance*.  
  https://openai.com/policies/how-your-data-is-used-to-improve-model-performance/

- Richardson, R., Schultz, J., & Crawford, K. (2019). *Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice*. NYU Law Review, 94, 192–233.

- United Nations Human Rights Council. (2024). *Racism and AI: Bias from the past leads to bias in the future*. Office of the High Commissioner for Human Rights.  
  https://www.ohchr.org/en/stories/2024/07/racism-and-ai-bias-past-leads-bias-future
